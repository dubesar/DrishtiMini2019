# -*- coding: utf-8 -*-
"""magictiele.pynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1AuNNp01e6BbjJVajXn2bJsm3I-WTH1Is
"""

#import libraries
import random
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from numpy import array
from numpy import argmax
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import normalize,scale

#defining function
def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))



  
  
def loss(h,y,m):
    return sum(sum(-y*np.log(h)-(1-y)*np.log(1-h)))/m
def grad(x):
  return sigmoid(x)*(1-sigmoid(x))

#importing data

data=pd.read_csv('D:\downloads\Magic Telescope.csv')

data.head()

#formating data
data=data.drop(columns=['ID'])


col=list(map((lambda x:x.replace(':','')),data.columns))
data.columns=col
data.columns

#data cleaning

a=(data[data['class']!='g'].index)
b=(data[data['class']!='h'].index)
c=a.intersection(b)
c=list(c)
c=array(c)
data=data.drop(c)
data=data.convert_objects(convert_numeric=True).dropna()
dat=(data.iloc[:,:-1])
dat=((dat)-(dat).min()+1).transform(np.log)
dat=np.array(dat)
dat=normalize(dat)
dat=scale(dat)

#cleaned data

x=dat

y=data.iloc[:,-1]
#encode 
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y=np.array(y)
y=y.reshape(-1,1)

# initializing our inputs and outputs
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test= train_test_split(x, y, test_size=0.33, random_state=33)


#adding bias to input
# X_train=np.hstack((np.ones((X_train.shape[0],1)),X_train))
# X_test=np.hstack((np.ones((X_test.shape[0],1)),X_test))




y_train=y_train.reshape(-1,1)
y_test=y_test.reshape(-1,1)


#initializing theta
#t=np.random.random(X_train[1]).reshape(X_train[1],1)
fea_num=X_train.shape[1]
t=np.zeros([fea_num,1])


z=np.dot(X_train,t)
a=(sigmoid(z))
a=np.round(a)

#algoritham
epoch=150
cost=[]
cost1=[]
for i in range(epoch):
    m=X_train.shape[0]
    z=np.dot(X_train,t)
    h=(sigmoid(z))
    l=loss(h,y_train,m)
    cost.append(l)
    l=loss((sigmoid(np.dot(X_test,t))),y_test,X_test.shape[0])
    cost1.append(l)
    
    dj=np.dot(X_train.T,h-y_train)/m
    alpha=0.8
    t=t-alpha*dj+0.01*1/m*t*t

#learning curve 
    
plt.scatter(range(epoch),cost)
plt.scatter(range(epoch),cost1)

#accuracy for test
for j in [0.01, 0.02, 0.05, 0.1, 0.2,0.5]:
    a=(sigmoid(np.dot(X_test,t)))
    for i in range(len(a)):
        if a[i]>=j:
            a[i]=1
        else:
            a[i]=0
            
    print ( 'for {} Accuracy from scratch: {}'.format(j,(a == y_test).sum().astype(float) / len(a)))
